"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import flyteidl.plugins.sagemaker.parameter_ranges_pb2
import flyteidl.plugins.sagemaker.training_job_pb2
import google.protobuf.descriptor
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class HyperparameterTuningJob(google.protobuf.message.Message):
    """A pass-through for SageMaker's hyperparameter tuning job"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRAINING_JOB_FIELD_NUMBER: builtins.int
    MAX_NUMBER_OF_TRAINING_JOBS_FIELD_NUMBER: builtins.int
    MAX_PARALLEL_TRAINING_JOBS_FIELD_NUMBER: builtins.int
    @property
    def training_job(self) -> flyteidl.plugins.sagemaker.training_job_pb2.TrainingJob:
        """The underlying training job that the hyperparameter tuning job will launch during the process"""
        pass
    max_number_of_training_jobs: builtins.int = ...
    """The maximum number of training jobs that an hpo job can launch. For resource limit purpose.
    https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ResourceLimits.html
    """

    max_parallel_training_jobs: builtins.int = ...
    """The maximum number of concurrent training job that an hpo job can launch
    https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ResourceLimits.html
    """

    def __init__(self,
        *,
        training_job : typing.Optional[flyteidl.plugins.sagemaker.training_job_pb2.TrainingJob] = ...,
        max_number_of_training_jobs : builtins.int = ...,
        max_parallel_training_jobs : builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal[u"training_job",b"training_job"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal[u"max_number_of_training_jobs",b"max_number_of_training_jobs",u"max_parallel_training_jobs",b"max_parallel_training_jobs",u"training_job",b"training_job"]) -> None: ...
global___HyperparameterTuningJob = HyperparameterTuningJob

class HyperparameterTuningObjectiveType(google.protobuf.message.Message):
    """HyperparameterTuningObjectiveType determines the direction of the tuning of the Hyperparameter Tuning Job
    with respect to the specified metric.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class Value(_Value, metaclass=_ValueEnumTypeWrapper):
        pass
    class _Value:
        V = typing.NewType('V', builtins.int)
    class _ValueEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Value.V], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        MINIMIZE = HyperparameterTuningObjectiveType.Value.V(0)
        MAXIMIZE = HyperparameterTuningObjectiveType.Value.V(1)

    MINIMIZE = HyperparameterTuningObjectiveType.Value.V(0)
    MAXIMIZE = HyperparameterTuningObjectiveType.Value.V(1)

    def __init__(self,
        ) -> None: ...
global___HyperparameterTuningObjectiveType = HyperparameterTuningObjectiveType

class HyperparameterTuningObjective(google.protobuf.message.Message):
    """The target metric and the objective of the hyperparameter tuning.
    https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    OBJECTIVE_TYPE_FIELD_NUMBER: builtins.int
    METRIC_NAME_FIELD_NUMBER: builtins.int
    objective_type: global___HyperparameterTuningObjectiveType.Value.V = ...
    """HyperparameterTuningObjectiveType determines the direction of the tuning of the Hyperparameter Tuning Job
    with respect to the specified metric.
    """

    metric_name: typing.Text = ...
    """The target metric name, which is the user-defined name of the metric specified in the
    training job's algorithm specification
    """

    def __init__(self,
        *,
        objective_type : global___HyperparameterTuningObjectiveType.Value.V = ...,
        metric_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal[u"metric_name",b"metric_name",u"objective_type",b"objective_type"]) -> None: ...
global___HyperparameterTuningObjective = HyperparameterTuningObjective

class HyperparameterTuningStrategy(google.protobuf.message.Message):
    """Setting the strategy used when searching in the hyperparameter space
    Refer this doc for more details:
    https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class Value(_Value, metaclass=_ValueEnumTypeWrapper):
        pass
    class _Value:
        V = typing.NewType('V', builtins.int)
    class _ValueEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Value.V], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        BAYESIAN = HyperparameterTuningStrategy.Value.V(0)
        RANDOM = HyperparameterTuningStrategy.Value.V(1)

    BAYESIAN = HyperparameterTuningStrategy.Value.V(0)
    RANDOM = HyperparameterTuningStrategy.Value.V(1)

    def __init__(self,
        ) -> None: ...
global___HyperparameterTuningStrategy = HyperparameterTuningStrategy

class TrainingJobEarlyStoppingType(google.protobuf.message.Message):
    """When the training jobs launched by the hyperparameter tuning job are not improving significantly,
    a hyperparameter tuning job can be stopping early.
    Note that there's only a subset of built-in algorithms that supports early stopping.
    see: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class Value(_Value, metaclass=_ValueEnumTypeWrapper):
        pass
    class _Value:
        V = typing.NewType('V', builtins.int)
    class _ValueEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Value.V], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        OFF = TrainingJobEarlyStoppingType.Value.V(0)
        AUTO = TrainingJobEarlyStoppingType.Value.V(1)

    OFF = TrainingJobEarlyStoppingType.Value.V(0)
    AUTO = TrainingJobEarlyStoppingType.Value.V(1)

    def __init__(self,
        ) -> None: ...
global___TrainingJobEarlyStoppingType = TrainingJobEarlyStoppingType

class HyperparameterTuningJobConfig(google.protobuf.message.Message):
    """The specification of the hyperparameter tuning process
    https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-tuning-job.html#automatic-model-tuning-ex-low-tuning-config
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    HYPERPARAMETER_RANGES_FIELD_NUMBER: builtins.int
    TUNING_STRATEGY_FIELD_NUMBER: builtins.int
    TUNING_OBJECTIVE_FIELD_NUMBER: builtins.int
    TRAINING_JOB_EARLY_STOPPING_TYPE_FIELD_NUMBER: builtins.int
    @property
    def hyperparameter_ranges(self) -> flyteidl.plugins.sagemaker.parameter_ranges_pb2.ParameterRanges:
        """ParameterRanges is a map that maps hyperparameter name to the corresponding hyperparameter range"""
        pass
    tuning_strategy: global___HyperparameterTuningStrategy.Value.V = ...
    """Setting the strategy used when searching in the hyperparameter space"""

    @property
    def tuning_objective(self) -> global___HyperparameterTuningObjective:
        """The target metric and the objective of the hyperparameter tuning."""
        pass
    training_job_early_stopping_type: global___TrainingJobEarlyStoppingType.Value.V = ...
    """When the training jobs launched by the hyperparameter tuning job are not improving significantly,
    a hyperparameter tuning job can be stopping early.
    """

    def __init__(self,
        *,
        hyperparameter_ranges : typing.Optional[flyteidl.plugins.sagemaker.parameter_ranges_pb2.ParameterRanges] = ...,
        tuning_strategy : global___HyperparameterTuningStrategy.Value.V = ...,
        tuning_objective : typing.Optional[global___HyperparameterTuningObjective] = ...,
        training_job_early_stopping_type : global___TrainingJobEarlyStoppingType.Value.V = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal[u"hyperparameter_ranges",b"hyperparameter_ranges",u"tuning_objective",b"tuning_objective"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal[u"hyperparameter_ranges",b"hyperparameter_ranges",u"training_job_early_stopping_type",b"training_job_early_stopping_type",u"tuning_objective",b"tuning_objective",u"tuning_strategy",b"tuning_strategy"]) -> None: ...
global___HyperparameterTuningJobConfig = HyperparameterTuningJobConfig
